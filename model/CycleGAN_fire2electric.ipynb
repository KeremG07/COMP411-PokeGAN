{"cells":[{"cell_type":"markdown","metadata":{"id":"TSwtbIn6xdl2"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlKsyOud3MpN"},"outputs":[],"source":["import torch \n","import torch.nn as nn\n","import os\n","from torch.utils.data import Dataset\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms, utils, datasets \n","from tqdm import tqdm\n","import torch.optim as optim\n","from torchvision.utils import save_image\n","from torch.utils.data import DataLoader\n","import torchvision\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import gc\n"]},{"cell_type":"markdown","metadata":{"id":"QCL6tfFx1NlA"},"source":["# Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36781,"status":"ok","timestamp":1674421878491,"user":{"displayName":"ATA SAYIN","userId":"06814481638654216562"},"user_tz":-180},"id":"B9lLjYk8Ei6x","outputId":"b1785b03-cbc8-4258-d35e-951f67109645"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"gzLjiEONvIPs"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2_HAcuhvJ-5"},"outputs":[],"source":["class SourceTargetDataset(Dataset):\n","  def __init__(self, root_source, root_target, transform = None):\n","    self.root_source = root_source\n","    self.root_target = root_target\n","    self.transform = transform\n","    \n","    self.images_source = os.listdir(self.root_source)\n","    self.images_target = os.listdir(self.root_target)\n","\n","    self.len_source = len(self.images_source)\n","    self.len_target = len(self.images_target)\n","\n","    self.length_dataset = max(self.len_source,self.len_target)\n","\n","  def __len__(self):\n","    return self.length_dataset \n","  \n","  def __getitem__(self, index):\n","    img_source = self.images_source[ index % self.len_source]\n","    img_target = self.images_target[ index % self.len_target]\n","\n","    path_source = os.path.join(self.root_source,img_source)\n","    path_target = os.path.join(self.root_target,img_target)\n","\n","    img_source = np.array(Image.open(path_source).convert(\"RGB\"))\n","    img_target = np.array(Image.open(path_target).convert(\"RGB\"))\n","\n","    if self.transform:\n","      img_source = self.transform(img_source)\n","      img_target = self.transform(img_target)\n","\n","    return img_source, img_target\n","\n","class TestDataset(Dataset):\n","  def __init__(self, root_source, transform = None):\n","    self.root_source = root_source\n","    self.transform = transform\n","    self.images_source = os.listdir(self.root_source)\n","    self.length_dataset = len(self.images_source)\n","\n","  def __len__(self):\n","    return self.length_dataset\n","  \n","  def __getitem__(self, index):\n","    img_source = self.images_source[index]\n","    path_source = os.path.join(self.root_source,img_source)\n","    img_source = np.array(Image.open(path_source).convert(\"RGB\"))\n","\n","    if self.transform:\n","      img_source = self.transform(img_source)\n","\n","    return img_source"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI5eWZrjoLju"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.CenterCrop(256),\n"," ])\n","\n","def show_images(images):\n","    images = np.reshape(images, [images.shape[0], -1]) \n","    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n","    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n","\n","    fig = plt.figure(figsize=(sqrtn, sqrtn))\n","    gs = gridspec.GridSpec(sqrtn, sqrtn)\n","    gs.update(wspace=0.05, hspace=0.05)\n","\n","    for i, img in enumerate(images):\n","        ax = plt.subplot(gs[i])\n","        plt.axis('off')\n","        ax.set_xticklabels([])\n","        ax.set_yticklabels([])\n","        ax.set_aspect('equal')\n","        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n","    return"]},{"cell_type":"markdown","metadata":{"id":"wtyVpbPt3SAL"},"source":["# Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tgEl4QE3QYx"},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self,in_channels,out_channels,stride):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 4, stride, bias=True, padding = 1, padding_mode=\"reflect\"),\n","            nn.InstanceNorm2d(out_channels),\n","            nn.LeakyReLU(0.2),\n","        )\n","    def forward(self,x):\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xuw6vY1I3bZC"},"outputs":[],"source":["class Discriminator(nn.Module):\n","  def __init__(self, in_channels=3, features = [64, 128, 256, 512]):\n","    super().__init__()\n","    layers = []\n","    layers.append(nn.Conv2d(in_channels, features[0], kernel_size = 4, stride= 2, padding=1, padding_mode= \"reflect\"))\n","\n","    in_channels = features[0]\n","    for feature in features[1:]:\n","      layers.append(Block(in_channels, feature, stride = 1 if feature==features[-1] else 2))\n","      in_channels = feature\n","\n","    layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\" ))\n","    self.model = nn.Sequential(*layers)\n","\n","  def forward(self,x):\n","    return torch.sigmoid(self.model(x))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YvqTHV5O-Jjo"},"source":["# Generator\n","\n","## c7s1-64, d128, d256, R256, R256, R256, R256, R256, R256, u128, u64, c7s1-3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLGJK0wd-K-x"},"outputs":[],"source":["class ConvBlock(nn.Module):\n","  def __init__(self, in_channels, out_channels, down = True, use_act = True, **kwargs):\n","    super().__init__()\n","\n","    self.conv = nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs) \n","        if down\n","        else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n","        \n","        nn.InstanceNorm2d(out_channels),\n","       \n","        nn.ReLU(inplace=True) if use_act  else nn.Identity(),\n","    )\n","\n","  def forward(self, x):\n","    return self.conv(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltqlWoBdeHhO"},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","  def __init__(self, channels):\n","    super().__init__()\n","\n","    self.conv = nn.Sequential(\n","        ConvBlock(channels,channels, kernel_size = 3, padding=1),\n","        ConvBlock(channels,channels, use_act=False, kernel_size = 3, padding=1)\n","    )\n","\n","  def forward(self, x):\n","    return x + self.conv(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lblHMlUgmjln"},"outputs":[],"source":["class Generator(nn.Module):\n","  def __init__(self, img_size=3, num_features = 64, num_residuals=6):\n","    super().__init__()\n","\n","    layers = []\n","    # c7s1-64\n","    self.initial = nn.Sequential(\n","        nn.Conv2d(img_size, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n","        nn.InstanceNorm2d(num_features=num_features),\n","        nn.ReLU(inplace=True)\n","    )\n","    layers.append(self.initial)\n","    \n","    # d128, d256\n","    self.down_blocks = nn.Sequential(\n","        ConvBlock(num_features,num_features*2, kernel_size=3, stride=2,padding=1),\n","        ConvBlock(num_features*2,num_features*4, kernel_size=3, stride=2, padding=1)\n","    )\n","    layers.append(self.down_blocks)\n","\n","    # R256, R256, R256, R256, R256, R256,\n","    self.res_blocks = nn.Sequential(\n","       *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n","    )\n","    layers.append(self.res_blocks)  \n","    \n","    # u128, u64\n","    self.up_blocks = nn.Sequential(\n","        ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n","        ConvBlock(num_features*2, num_features, down=False, kernel_size=3, stride=2, padding=1, output_padding=1)\n","    )\n","    layers.append(self.up_blocks)\n","\n","    # c7s1-3\n","    self.out_block = nn.Sequential(\n","        nn.Conv2d(num_features, 3, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n","        nn.InstanceNorm2d(num_features=3),\n","    )\n","    layers.append(self.out_block)\n","\n","    self.model = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    return torch.tanh(self.model(x))"]},{"cell_type":"markdown","metadata":{"id":"XmZgS_QvuI35"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1674421906234,"user":{"displayName":"ATA SAYIN","userId":"06814481638654216562"},"user_tz":-180},"id":"rql-FSwVuIr-","outputId":"43ab15ba-0334-4225-a0be-8a7930124d40"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","#DEVICE = \"cpu\"\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 4\n","NUM_WORKERS = 2\n","NUM_EPOCHS = 22\n","LAMBDA_CYCLE = 10\n","LAMBDA_IDENTITY = 0.5 * LAMBDA_CYCLE\n","DEVICE"]},{"cell_type":"markdown","metadata":{"id":"do2rxu91oiAg"},"source":[" # Paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yNi3RIknGYE"},"outputs":[],"source":["DRIVE_MAIN_FOLDER = \"/content/drive/MyDrive/Fall22/COMP411/Project/\"\n","\n","# Data\n","DRIVE_DATASET_FOLDER = DRIVE_MAIN_FOLDER + \"dataset/\"\n","\n","FIRE_TRAIN_PATH = DRIVE_DATASET_FOLDER + \"Fire/\"\n","ELECTRIC_TRAIN_PATH = DRIVE_DATASET_FOLDER + \"Electric/\"\n","FIRE_TEST_PATH = DRIVE_DATASET_FOLDER + \"testFire/\"\n","ELECTRIC_TEST_PATH = DRIVE_DATASET_FOLDER + \"testElectric/\"\n","\n","# MODELS\n","MODEL_CHECK_DIR = DRIVE_MAIN_FOLDER + \"/models/\"\n","\n","# Test Outputs\n","TEST_PATH = DRIVE_MAIN_FOLDER + \"/testOutputs/\"\n"]},{"cell_type":"markdown","metadata":{"id":"UjH8LZ34_f_C"},"source":["# Train "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-uS6Pnj_hQk"},"outputs":[],"source":["def train(disc_X, disc_Y, gen_X, gen_Y, loader, optim_disc, optim_gen,loss_histroy):\n","  loop = tqdm(loader, leave = True, position=0)\n","\n","  l1 = nn.L1Loss()\n","  mse = nn.MSELoss()\n","  \n","  if DEVICE == \"cuda\":\n","    g_scaler = torch.cuda.amp.GradScaler()\n","    d_scaler = torch.cuda.amp.GradScaler()\n","    \n","  for idx, (x, y) in enumerate(loop):\n","    x = x.to(DEVICE)\n","    y = y.to(DEVICE)\n","   \n","   # Discriminator loss\n","    with torch.cuda.amp.autocast():\n","      # Discriminator - Y\n","      optim_disc.zero_grad()\n","\n","      D_y_real = disc_Y(y)\n","      D_y_real_loss = mse(D_y_real, torch.ones_like(D_y_real))\n","\n","      fake_y = gen_Y(x)\n","      D_y_fake = disc_Y(fake_y.detach())\n","      D_y_fake_loss = mse(D_y_fake, torch.zeros_like(D_y_fake))\n","\n","      D_y_loss = D_y_real_loss + D_y_fake_loss\n","\n","\n","      # Discriminator - X\n","      D_x_real = disc_X(x)\n","      D_x_real_loss = mse(D_x_real, torch.ones_like(D_x_real))\n","\n","      fake_x = gen_X(y)\n","      D_x_fake = disc_X(fake_x.detach())\n","      D_x_fake_loss = mse(D_x_fake, torch.zeros_like(D_x_fake))\n","\n","      D_x_loss = D_x_real_loss + D_x_fake_loss\n","\n","      # Discriminator - Total\n","      D_loss = D_y_loss + D_x_loss \n","      print(f\"\\nDiscriminator Losses -> Total Loss: {D_loss: .3f}, D_x loss: {D_x_loss: .3f}, D_y loss {D_y_loss: .3f}\")\n","      \n","      if DEVICE == \"cuda\":\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(optim_disc)\n","        d_scaler.update()\n","      else:\n","        D_loss.backward()\n","        optim_disc.step()\n","\n","      loss_history[\"D_total\"].append(D_loss.data )\n","\n","    # Generator loss\n","    with torch.cuda.amp.autocast():\n","      # Adversarial Loss\n","      optim_gen.zero_grad()\n","      \n","      # L_GAN(G_Y, D_Y , X, Y) \n","      # = Ey∼pdata(y) [log D_Y(y)]\n","      # + Ex∼pdata(x) [log(1 − D_Y(G_Y(x))]\n","      \n","      # L_GAN(G_X, D_X , Y, X) \n","      # = Ex∼pdata(x) [log D_X(x)]\n","      # + Ey∼pdata(y) [log(1 − D_X(G_X(y))]\n","\n","      D_x_fake = disc_X(fake_x)\n","      D_y_fake = disc_Y(fake_y)\n","\n","      G_x_loss = mse(D_x_fake, torch.ones_like(D_x_fake))\n","      G_y_loss = mse(D_y_fake, torch.ones_like(D_y_fake))\n","\n","\n","      # Cycle Consistency Loss\n","      # L_cyc (G_Y, G_X)\n","      # = Ex∼pdata(x) [|G_X(G_Y(x)) − x|_1]\n","      # + Ey∼pdata(y) [|G_Y(G_X(y)) − y|_1]\n","\n","      cycle_x = gen_X(fake_y)\n","      cycle_y = gen_Y(fake_x)\n","\n","      cycle_x_loss = l1(cycle_x,x)\n","      cycle_y_loss = l1(cycle_y,y)\n","\n","      cycle_loss = cycle_x_loss + cycle_y_loss\n","\n","      # Identity Loss\n","      # L_identity(G,F) \n","      # = Ey∼pdata(y)[|G_Y(y) − y|_1] \n","      # + Ex∼pdata(x) [|G_X(x) − x|_1 ]\n","\n","      identity_x_loss = l1(gen_X(x),x)\n","      identity_y_loss = l1(gen_Y(y),y)\n","      \n","      identity_loss = identity_x_loss + identity_y_loss\n","\n","      # Total loss\n","      G_loss = G_x_loss + G_y_loss + cycle_loss * LAMBDA_CYCLE + identity_loss * LAMBDA_IDENTITY \n","      print(f\"Generator Losses -> Total Loss: {G_loss: .3f}, G_x loss: {G_x_loss: .3f}, G_y loss {G_y_loss: .3f}, Cycle loss: {cycle_loss: .3f}, Identity loss: {identity_loss: .3f}\")\n","\n","      if DEVICE == \"cuda\":\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(optim_gen)\n","        g_scaler.update() \n","      else:\n","        G_loss.backward()\n","        optim_gen.step()\n","\n","    \n","      loss_history[\"G_total\"].append(G_loss.data)\n","      loss_history[\"G_x\"].append(G_x_loss.data)\n","      loss_history[\"G_y\"].append(G_y_loss)\n","      loss_history[\"G_cycle\"].append(cycle_loss.data)\n","      loss_history[\"G_identity\"].append(identity_loss.data)"]},{"cell_type":"markdown","metadata":{"id":"FijBUi4eilvO"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSZkTNb9zIda"},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","\n","disc_X = Discriminator().to(DEVICE)\n","disc_Y = Discriminator().to(DEVICE)\n","gen_Y = Generator().to(DEVICE)\n","gen_X = Generator().to(DEVICE)\n","\n","disc_X.load_state_dict(torch.load(MODEL_CHECK_DIR+\"fire2elec_discriminator_X_param.pkl\"))\n","disc_Y.load_state_dict(torch.load(MODEL_CHECK_DIR+\"fire2elec_discriminator_Y_param.pkl\"))\n","gen_X.load_state_dict(torch.load(MODEL_CHECK_DIR+\"fire2elec_generator_X_param.pkl\"))\n","gen_Y.load_state_dict(torch.load(MODEL_CHECK_DIR+\"fire2elec_generator_Y_param.pkl\"))\n","\n","loss_history = dict.fromkeys(['D_total','G_total','G_x','G_y','G_cycle','G_identity'],[])\n","\n","torch.manual_seed(130)\n","\n","opt_disc = optim.Adam(\n","    list(disc_X.parameters()) + list(disc_Y.parameters()),\n","    lr=LEARNING_RATE,\n","    betas=(0.5, 0.999),\n",")\n","\n","opt_gen = optim.Adam(\n","    list(gen_Y.parameters()) + list(gen_X.parameters()),\n","    lr=LEARNING_RATE,\n","    betas=(0.5, 0.999),\n",")\n","\n","dataset = SourceTargetDataset(\n","    root_source=FIRE_TRAIN_PATH, \n","    root_target=ELECTRIC_TRAIN_PATH, \n","    transform=transform\n",")\n","\n","loader = DataLoader(\n","    dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=True\n",")\n","print(f\"lr: {LEARNING_RATE},batch size: {BATCH_SIZE},lambda cycle: {LAMBDA_CYCLE}, lambda identity: {LAMBDA_IDENTITY}\")\n","\n","for epoch in range(NUM_EPOCHS):\n","  print(f\"\\nEpoch #{epoch}\\n\")\n","  train(disc_X, disc_Y, gen_X, gen_Y, loader, opt_disc, opt_gen, loss_history)\n","  torch.save(gen_X.state_dict(), MODEL_CHECK_DIR + 'fire2elec_generator_X_param.pkl')\n","  torch.save(gen_Y.state_dict(), MODEL_CHECK_DIR + 'fire2elec_generator_Y_param.pkl')\n","  torch.save(disc_X.state_dict(), MODEL_CHECK_DIR + 'fire2elec_discriminator_X_param.pkl')\n","  torch.save(disc_Y.state_dict(), MODEL_CHECK_DIR + 'fire2elec_discriminator_Y_param.pkl')\n"]},{"cell_type":"markdown","metadata":{"id":"hetOC9ockcO4"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41315,"status":"ok","timestamp":1674422257507,"user":{"displayName":"ATA SAYIN","userId":"06814481638654216562"},"user_tz":-180},"id":"QdKq6uMJYZrn","outputId":"8b42d514-d918-42e5-a8f5-bfd7d6e8102d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 49/49 [00:40<00:00,  1.22it/s]\n"]}],"source":["from torch.serialization import MAP_LOCATION\n","test_dataset_f = TestDataset(\n","    root_source=FIRE_TEST_PATH,\n","    transform = transform,\n",")\n","\n","test_loader = DataLoader(\n","        test_dataset_f,\n","        batch_size=1,\n","        shuffle=False,\n","        pin_memory=True,\n","    )\n","\n","gen_Y = Generator()\n","gen_Y.load_state_dict(torch.load(MODEL_CHECK_DIR+\"fire2elec_generator_X_param.pkl\",map_location=torch.device('cpu')))\n","\n","with torch.no_grad():\n","  loop = tqdm(test_loader, leave=True, position = 0)\n","  for idx, x in enumerate(loop):\n","    #y = gen_Y(x)\n","    y = x\n","    save_image(y, f\"{TEST_PATH}/test{idx}.png\")\n","  "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TSwtbIn6xdl2","gzLjiEONvIPs","wtyVpbPt3SAL","YvqTHV5O-Jjo","UjH8LZ34_f_C"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}